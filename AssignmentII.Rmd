---
title: "Advanced Topics in Regression - Assignment II"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## (a)

## b 

### i

```{r}
library(ggplot2)
library(tidyverse)
library(dplyr)

rm(list = ls())
filepath = "data/tempexam.csv"
temp_data = read.csv2(filepath)
summary(temp_data)

# Nadayara-Watson function
gaussian_kernel = function(x,h){
  return ((1/sqrt(2*pi*(h^2))) * exp(-0.5*(x/h)^2))
}

nw_estimator = function(x, observed_x, observed_y, h){
 weights = sapply(observed_x, function(xi) gaussian_kernel(abs(x-xi), h))
 weights = weights/sum(weights) # normalize weights
 y_hat = sum(weights * observed_y)
 return (y_hat)
}

x_train = as.matrix(temp_data$Dates)
y_train = as.matrix(as.numeric(temp_data$y))

y_hat = sapply(x_train, nw_estimator,observed_x = x_train, observed_y = y_train, 0.5)
final_temp_data = data.frame(x= x_train, y = y_train, y_hat=y_hat)

# Plot Nadayara-Watson estimators against the original data
ggplot(final_temp_data, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  geom_line(aes(y = y_hat), color = "red") +
  labs(title = "Nadaraya-Watson Regression Smoothing",
       x = "Year",
       y = "Temperature")
```

### ii

```{r}
# Leave one-out-cross-validation

loo_cv = function(data){
 cv_grid = matrix(data=NA, nrow=nrow(data), ncol = 31)
 cv_error_grid = matrix(data=NA, nrow=nrow(data), ncol = 31)
 for (i in 1:nrow(data)){
   training_set = data[-i, ]
   validation_set = data[i, ]
   x_train = as.matrix(data$Dates)
   y_train = as.matrix(as.numeric(data$y))
   x_validation = as.matrix(validation_set$Dates)
   y_validation = as.matrix(as.numeric(validation_set$y))
   cv_grid[i,1] = x_validation
   cv_error_grid[i, 1] = x_validation
   for( h in 2:31){
     y_hat = sapply(x_validation, nw_estimator,observed_x = x_train, observed_y = y_train, h)
     cv_grid[i,h] = y_hat
     cv_error_grid[i, h] = (y_validation - y_hat)^2
   }
 }
 colnames(cv_grid) = c("x", paste0("h", 1:30))
 mse = apply(cv_error_grid[,-1], 2, mean)
 return (list(fitted_values = cv_grid, errors = cv_error_grid, mse=mse))
}

loo_cv_result = loo_cv(temp_data)
cv_errors = loo_cv_result$errors
cv_mse = loo_cv_result$mse
plot(cv_mse, type = "l")
optimal_bandwith = which.min(cv_mse)

# Use optimal bandwith to plot estimate
y_hat = sapply(x_train, nw_estimator,observed_x = x_train, observed_y = y_train, optimal_bandwith)
final_temp_data = data.frame(x= x_train, y = y_train, y_hat=y_hat)

# Plot Nadayara-Watson estimators against the original data
ggplot(final_temp_data, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  geom_line(aes(y = y_hat), color = "red") +
  labs(title = "Nadaraya-Watson Regression Smoothing",
       x = "Year",
       y = "Temperature")
```


# Question 2

```{r}

# regopt <- function(tuning, Y, X, nitermax=20){
# #tuning=[lambda1, lambda2] = tuning parameters in this order
# #Y and X = the response and the design matrix
# #nitermax = the number of iterations used to
# #evaluate the function that is being minimised (i.e. A)
# #put some code here if required
# #return these objects
# #errortol is some error tolerance value
# list(errortol=errortol, beta=beta, A=A)
#}

regopt = function(tuning, Y, X, nitermax=20){
  lamba1 = tuning[1]
  lambda2 = tuning[2]
  
  # Gradient Descent Learning Rate hyperparameter 
  alpha = 0.01 # Learning rate
  A = NULL
  if(all(tuning==c(0,0))){
    # compute OLS
    A = compute_ols(tuning, Y,X, beta)
  
  }else {
     for (i in 1:nitermax) {
       beta = beta_gradient_desc(tuning, Y, X, alpha) 
     }
    A = elastic_net_obj_function(tuning, Y,X, beta)
  }
  errortol = t((Y-X%*%beta))%*%(Y-X%*%beta)
  return(list(errortol= errortol, beta = beta, A = A))
  
}

compute_ols = function(tuning, Y,X, beta){
  
}

elastic_net_obj_function = function(tuning, Y,X, beta){
  lamba1 = tuning[1]
  lambda2 = tuning[2]
  one_transponse = t(matrix(1, nrow=nrows(X), ncol=ncol(X)))
  rss = t((Y-X%*%beta))%*%(Y-X%*%beta)
  lasso_penalty = lambda1 * one_transponse * sign(beta)
  ridge_penalty = lambda2 * t(beta)%*%beta
  A = rss + lasso_penalty + ridge_penalty
  return(A)
}
  
beta_gradient_desc = function(tuning, Y,X, beta, alpha){
  lamba1 = tuning[1]
  lambda2 = tuning[2]
  gradient_rss   = -2*t(X) %*%Y + 2*t(X)%*%X%*%beta
  gradient_lasso = lamba1 * t(matrix(1, nrow=nrows(X), ncol=ncol(X)))%*%sign(beta)
  gradient_ridge = 2*lambda2 * beta
  beta_old_slope = gradient_rss + gradient_lasso + gradient_ridge
  beta_new = beta -  (alpha*beta_old_slope)
  return (beta_new)
}

```