---
title: "Advanced Topics in Regression - Assignment II"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## (a)

## b 

### i

```{r}
library(ggplot2)
library(tidyverse)
library(dplyr)

rm(list = ls())
filepath = "data/tempexam.csv"
temp_data = read.csv2(filepath)
summary(temp_data)

# Nadayara-Watson function
gaussian_kernel = function(x,h){
  return ((1/sqrt(2*pi*(h^2))) * exp(-0.5*(x/h)^2))
}

nw_estimator = function(x, observed_x, observed_y, h){
 weights = sapply(observed_x, function(xi) gaussian_kernel(abs(x-xi), h))
 weights = weights/sum(weights) # normalize weights
 y_hat = sum(weights * observed_y)
 return (y_hat)
}

x_train = as.matrix(temp_data$Dates)
y_train = as.matrix(as.numeric(temp_data$y))

y_hat = sapply(x_train, nw_estimator,observed_x = x_train, observed_y = y_train, 0.5)
final_temp_data = data.frame(x= x_train, y = y_train, y_hat=y_hat)

# Plot Nadayara-Watson estimators against the original data
ggplot(final_temp_data, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  geom_line(aes(y = y_hat), color = "red") +
  labs(title = "Nadaraya-Watson Regression Smoothing",
       x = "Year",
       y = "Temperature")
```

### ii

```{r}
# Leave one-out-cross-validation

loo_cv = function(data){
 cv_grid = matrix(data=NA, nrow=nrow(data), ncol = 31)
 cv_error_grid = matrix(data=NA, nrow=nrow(data), ncol = 31)
 for (i in 1:nrow(data)){
   training_set = data[-i, ]
   validation_set = data[i, ]
   x_train = as.matrix(data$Dates)
   y_train = as.matrix(as.numeric(data$y))
   x_validation = as.matrix(validation_set$Dates)
   y_validation = as.matrix(as.numeric(validation_set$y))
   cv_grid[i,1] = x_validation
   cv_error_grid[i, 1] = x_validation
   for( h in 2:31){
     y_hat = sapply(x_validation, nw_estimator,observed_x = x_train, observed_y = y_train, h)
     cv_grid[i,h] = y_hat
     cv_error_grid[i, h] = (y_validation - y_hat)^2
   }
 }
 colnames(cv_grid) = c("x", paste0("h", 1:30))
 mse = apply(cv_error_grid[,-1], 2, mean)
 return (list(fitted_values = cv_grid, errors = cv_error_grid, mse=mse))
}

loo_cv_result = loo_cv(temp_data)
fitted_vals = loo_cv_result$fitted_values
cv_errors = loo_cv_result$errors
cv_mse = loo_cv_result$mse
plot(cv_mse, type = "l")
optimal_bandwith = which.min(cv_mse)

# Use optimal bandwith to plot estimate
y_hat = sapply(x_train, nw_estimator,observed_x = x_train, observed_y = y_train, optimal_bandwith)
final_temp_data = data.frame(x= x_train, y = y_train, y_hat=y_hat)

# Plot Nadayara-Watson estimators against the original data
ggplot(final_temp_data, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  geom_line(aes(y = y_hat), color = "red") +
  labs(title = "Nadaraya-Watson Regression Smoothing",
       x = "Year",
       y = "Temperature")
```


# Question 2

## b(i)

```{r}
library(tidyverse)

rm(list = ls())
prostate_data = read.table("data/prostate_data.txt")
head(prostate_data)
in_sample_obs = prostate_data%>%as_tibble() %>%
                filter(train==TRUE)
out_of_sample_obs = prostate_data%>%as_tibble() %>%
                filter(train==FALSE)

regopt = function(tuning, Y, X, nitermax=20){
  lamba1 = tuning[1]
  lambda2 = tuning[2]
  
  # Gradient Descent Learning Rate hyperparameter 
  alpha = 0.001 # Learning rate
  p = ncol(X)
  beta  = rep(0,p)
  
  A = NULL
  if(all(tuning==c(0,0))){
    # compute OLS Beta
    beta = ols_beta(Y,X)
  
  }else {
     for (i in 1:nitermax) {
       beta = beta_gradient_desc(tuning, Y, X, beta,alpha) 
     }
  }
  A = elastic_net_obj_function(tuning, Y,X, beta)
  errortol = t((Y-X%*%beta))%*%(Y-X%*%beta) # This is the residual sum of of squares error
  return(list(errortol= errortol, beta = beta, A = A))
  
}

ols_beta = function(Y,X){
  beta_hat = solve(t(X)%*%X)%*%t(X)%*%Y
  return (beta_hat)
}

elastic_net_obj_function = function(tuning, Y,X, beta){
  lambda1 = tuning[1]
  lambda2 = tuning[2]
  one_transponse = t(matrix(1, nrow=length(beta), ncol=1))
  rss = t((Y-X%*%beta))%*%(Y-X%*%beta)
  lasso_penalty = lambda1 * one_transponse %*% abs(beta)
  ridge_penalty = lambda2 * t(beta)%*%beta
  A = rss + lasso_penalty + ridge_penalty
  return(A)
}
  
beta_gradient_desc = function(tuning, Y,X, beta, alpha){
  lambda1 = tuning[1]
  lambda2 = tuning[2]
  gradient_rss   = -2*(t(X) %*%Y) + 2*(t(X)%*%X%*%beta)
  gradient_lasso = lambda1 * sign(beta)
  gradient_ridge = 2*lambda2 * beta
  beta_old_slope = gradient_rss + gradient_lasso + gradient_ridge
  beta_new = beta -  (alpha*beta_old_slope)
  return (beta_new)
}



in_sample_obs_std = scale(in_sample_obs[,-10], center=TRUE, scale=TRUE)%>%as_tibble()
Y_train = as.matrix(in_sample_obs_std$lpsa)
colnames(Y_train) = "lpsa"
X_train = as.matrix(in_sample_obs_std[, -9])
# Execute regopt
tuning <- c(0, 0)
result <- regopt(tuning, Y_train, X_train, nitermax = 100)
print(result)
```

## b(ii)

```{r}
loo_cv = function(Y,X, lasso_regression=TRUE){
  
  n = nrow(Y)
  # Lambda range [0,5] across 100 points
  lambda_range = seq(0,5, length.out=100)
  # Setup up cross-validation grid matrix
  cv_grid = matrix(data=NA, nrow=nrow(X), ncol = length(lambda_range)) 
  # Setup cross validation error grid matrix
  cv_error_grid = matrix(data=NA, nrow=nrow(X), ncol = length(lambda_range))
  
  for( i in 1:n){
    # Leave one observation out
    x_training_set = X[-i,]
    x_validation_set = X[i,]
    y_training_set  = Y[-i,]
    y_validation_set = Y[i,]
    for (j in 1: length(lambda_range)){
      tuning = c(0, 0)
      result = NA
      lambda = lambda_range[j]
      if(lasso_regression){
        tuning = c(lambda, 0)
        result = regopt(tuning, y_training_set, x_training_set, nitermax = 20)
      }else{
        tuning = c(0, lambda)
        result = regopt(tuning, y_training_set, x_training_set, nitermax = 20)
      }
       y_hat = x_validation_set%*%result$beta
       cv_grid[i,j] = y_hat
       cv_error_grid[i, j] = (y_validation_set - y_hat)^2
    }
  }
  colnames(cv_grid) = c(paste0("lambda", c(lambda_range)))
  colnames(cv_error_grid) = c(paste0("lambda", c(lambda_range)))
  mse = apply(cv_error_grid, 2, mean)
  return (list(fitted_values = cv_grid, errors = cv_error_grid, mse=mse))
}

# Cross-validation for Lasso Regression
lasso_loo_cv_result = loo_cv(Y_train,X_train,lasso_regression=FALSE)
lasso_reg_fitted_vals = lasso_loo_cv_result$fitted_values
lasso_reg_errors = lasso_loo_cv_result$errors
lasso_reg_mse = lasso_loo_cv_result$mse
optimal_lasso_lambda = which.min(lasso_reg_mse)
optimal_lasso_lambda 

# Cross-validation for Ridge Regression
ridge_loo_cv_result = loo_cv(Y_train,X_train,lasso_regression=FALSE)
ridge_reg_fitted_vals = ridge_loo_cv_result$fitted_values
ridge_reg_errors = ridge_loo_cv_result$errors
ridge_reg_mse = ridge_loo_cv_result$mse
optimal_ridge_lambda = which.min(ridge_reg_mse)
optimal_ridge_lambda 
```